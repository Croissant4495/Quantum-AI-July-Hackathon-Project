{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e574be3e",
   "metadata": {},
   "source": [
    "### Team 3\n",
    "#### Waether Forecasting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a665ebf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.metrics import mean_squared_error, root_mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35028f8b",
   "metadata": {},
   "source": [
    "Since we can't include all 32 features, we will try to see which features are most important"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46915297",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "df = pd.read_csv(\"Cairo-Weather.csv\")\n",
    "df_cleaned = df.drop(columns=[col for col in df.columns if '_min' in col or '_max' in col])\n",
    "\n",
    "# Drop non-numeric or non-predictive columns\n",
    "X_wind = df_cleaned.drop(columns=[\n",
    "    'time',  \n",
    "    'temperature_2m_mean (°C)',\n",
    "    'precipitation_sum (mm)' ,'sunrise (iso8601)','sunset (iso8601)',\n",
    "    'wind_speed_10m_mean (km/h)'\n",
    "])\n",
    "\n",
    "\n",
    "Y_wind = df[['temperature_2m_mean (°C)', 'precipitation_sum (mm)', 'wind_speed_10m_mean (km/h)']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ef2ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor()\n",
    "model.fit(X_wind, Y_wind['temperature_2m_mean (°C)'])\n",
    "\n",
    "importances = model.feature_importances_\n",
    "temp_features = pd.Series(importances, index=X_wind.columns).sort_values(ascending=False).head(5).index\n",
    "print(temp_features)\n",
    "\n",
    "model.fit(X_wind, Y_wind['precipitation_sum (mm)'])\n",
    "\n",
    "importances = model.feature_importances_\n",
    "percipitation_features = pd.Series(importances, index=X_wind.columns).sort_values(ascending=False).head(5).index\n",
    "print(percipitation_features)\n",
    "\n",
    "model.fit(X_wind, Y_wind['wind_speed_10m_mean (km/h)'])\n",
    "\n",
    "importances = model.feature_importances_\n",
    "wind_features = pd.Series(importances, index=X_wind.columns).sort_values(ascending=False).head(5).index\n",
    "print(wind_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0597f141",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv(\"Cairo-Weather.csv\")\n",
    "\n",
    "# Select only the target columns\n",
    "targets = df[['temperature_2m_mean (°C)', 'precipitation_sum (mm)', 'wind_speed_10m_mean (km/h)']]\n",
    "\n",
    "# Calculate correlation matrix\n",
    "corr_matrix = targets.corr()\n",
    "\n",
    "# Display correlation matrix\n",
    "print(corr_matrix)\n",
    "\n",
    "# Optional: visualize with heatmap\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm')\n",
    "plt.title(\"Correlation between target features\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c19ff0",
   "metadata": {},
   "source": [
    "Based on the correlation analysis between the target variables (`temperature_2m_mean`, `precipitation_sum`, and `wind_speed_10m_mean`), we observed relatively low correlation among them. This suggests that each target is influenced by different features to a significant extent.\n",
    "\n",
    "As a result, we will create **three separate regression models**, each dedicated to predicting one of the target variables. For each model, we will focus on selecting the top features that are most strongly correlated with the specific target, in order to improve accuracy and interpretability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8536d7c4",
   "metadata": {},
   "source": [
    "##### Temperature model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2a5d508",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data of Temperature model\n",
    "\n",
    "df = pd.read_csv(\"Cairo-Weather.csv\")\n",
    "\n",
    "\n",
    "# --- Step 1: Select 4 input features (X) ---\n",
    "X_wind = df[temp_features]\n",
    "\n",
    "# --- Step 2: Select 3 target variables (Y) ---\n",
    "Y_wind = df['temperature_2m_mean (°C)']\n",
    "\n",
    "# Split data into training and testing sets (80/20)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_wind, Y_wind, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale input features to range [-1, 1]\n",
    "xscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_temp_train_scaled = xscaler.fit_transform(X_train)\n",
    "X_temp_test_scaled = xscaler.transform(X_test)\n",
    "\n",
    "# Scale target values to range [-1, 1]\n",
    "yscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Reshape target arrays to 2D for the scaler\n",
    "Y_temp_train_scaled = yscaler.fit_transform(Y_train.values.reshape(-1, 1))\n",
    "Y_temp_test_scaled = yscaler.transform(Y_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57bfe53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Qiskit circuits & new V2 primitives\n",
    "from qiskit.circuit.library import PauliFeatureMap, EfficientSU2, ZZFeatureMap\n",
    "from qiskit.primitives import StatevectorEstimator, StatevectorSampler\n",
    "\n",
    "# Qiskit‑Machine‑Learning\n",
    "from qiskit_machine_learning.neural_networks import EstimatorQNN, SamplerQNN\n",
    "from qiskit_machine_learning.algorithms import NeuralNetworkRegressor\n",
    "from qiskit_machine_learning.optimizers import L_BFGS_B, COBYLA, ADAM, SPSA\n",
    "from qiskit_machine_learning.circuit.library import QNNCircuit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fe35f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an array to store evaluations of the objective function during optimization\n",
    "objective_func_vals = []\n",
    "\n",
    "# Define the callback function\n",
    "def callback_graph(*args):\n",
    "    \"\"\"\n",
    "    Universal callback for NeuralNetworkRegressor / SPSA / ADAM, etc.\n",
    "    It will be called with one of these argument‐patterns:\n",
    "      - (weights, loss)\n",
    "      - (nfev, weights, loss)\n",
    "      - (loss,)\n",
    "      - (nfev, weights, loss, grad)\n",
    "    This will extract the loss (the last numeric argument),\n",
    "    append it to `objective_func_vals`, and print it.\n",
    "    \"\"\"\n",
    "    # find the first float in args (assuming loss is a float)\n",
    "    loss = None\n",
    "    for a in args:\n",
    "        if isinstance(a, float):\n",
    "            loss = a\n",
    "    if loss is None:\n",
    "        # fallback: assume the last arg is loss\n",
    "        loss = args[-1]\n",
    "    \n",
    "    # Store loss\n",
    "    objective_func_vals.append(loss)   \n",
    "    iteration = len(objective_func_vals)\n",
    "\n",
    "    # Print every 10 iterations\n",
    "    if iteration % 10 == 0:\n",
    "        print(f\"Iteration {iteration:3d} — Loss: {loss:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fecf7e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Use the new V2 Estimator for expectation‑value QNNs\n",
    "estimator = StatevectorEstimator()\n",
    "\n",
    "# 2) (Optional) If you ever want a sampling‑based QNN, use StatevectorSampler + SamplerQNN\n",
    "# sampler = StatevectorSampler()\n",
    "# qnn = SamplerQNN(circuit=qc, sampler=sampler, input_gradients=True)\n",
    "\n",
    "# Feature dimension\n",
    "feature_dim = X_temp_train_scaled.shape[1]\n",
    "\n",
    "# --- 1. Build the QNN circuit ---\n",
    "\n",
    "feature_map = ZZFeatureMap(feature_dimension=feature_dim, reps=1)\n",
    "ansatz      = EfficientSU2(num_qubits=feature_dim, reps=2, entanglement=\"linear\" , insert_barriers=True)\n",
    "qc = QNNCircuit(feature_map=feature_map, ansatz=ansatz)\n",
    "\n",
    "# --- 2. Wrap into an EstimatorQNN using the V2 primitive ---\n",
    "\n",
    "qnn = EstimatorQNN(\n",
    "    circuit=qc,\n",
    "    estimator=estimator,\n",
    "    input_gradients=True           # enable analytic gradients\n",
    ")\n",
    "\n",
    "# --- 3. Build the sklearn‑style regressor ---\n",
    "\n",
    "regressor = NeuralNetworkRegressor(\n",
    "    neural_network=qnn,\n",
    "    loss=\"squared_error\",         # mean squared error\n",
    "    optimizer=SPSA(maxiter=150, callback=callback_graph),\n",
    "    # callback=callback_graph,       # populates your objective_func_vals list\n",
    ")\n",
    "\n",
    "objective_func_vals = []\n",
    "\n",
    "# --- 4. Train ---\n",
    "\n",
    "start = time.time()\n",
    "regressor.fit(X_temp_train_scaled, Y_temp_train_scaled)\n",
    "elapsed = time.time() - start\n",
    "print(f\"Training time: {elapsed:.1f} s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b09232",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (6, 4)\n",
    "plt.plot(objective_func_vals)\n",
    "plt.title(\"Objective Function Over Iterations\")\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99265e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 5. Predict & Evaluate ---\n",
    "\n",
    "y_pred = regressor.predict(X_temp_test_scaled)\n",
    "plt.figure(figsize=(8,5))\n",
    "plt.scatter(X_temp_test_scaled, Y_temp_test_scaled, color='blue',  label='True')\n",
    "plt.scatter(X_temp_test_scaled, y_pred, color='red',   label='Predicted')\n",
    "plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "print(f\"R² Train score: {regressor.score(X_temp_train_scaled, Y_temp_train_scaled):.4f}\")\n",
    "print(f\"R² Test score: {regressor.score(X_temp_test_scaled, Y_temp_test_scaled):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f67c7ddb",
   "metadata": {},
   "source": [
    "##### Precipitation Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc073102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data of Precipitation model\n",
    "\n",
    "df = pd.read_csv(\"Cairo-Weather.csv\")\n",
    "\n",
    "\n",
    "# --- Step 1: Select 4 input features (X) ---\n",
    "X_wind = df[percipitation_features]\n",
    "\n",
    "# --- Step 2: Select 3 target variables (Y) ---\n",
    "Y_wind = df['precipitation_sum (mm)']\n",
    "\n",
    "# Split data into training and testing sets (80/20)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_wind, Y_wind, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale input features to range [-1, 1]\n",
    "xscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_wind_train_scaled = xscaler.fit_transform(X_train)\n",
    "X_wind_test_scaled = xscaler.transform(X_test)\n",
    "\n",
    "# Scale target values to range [-1, 1]\n",
    "yscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Reshape target arrays to 2D for the scaler\n",
    "Y_wind_train_scaled = yscaler.fit_transform(Y_train.values.reshape(-1, 1))\n",
    "Y_wind_test_scaled = yscaler.transform(Y_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c23e2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9aa0b925",
   "metadata": {},
   "source": [
    "##### Wind Speed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c06c86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data of Wind Speed model\n",
    "\n",
    "df = pd.read_csv(\"Cairo-Weather.csv\")\n",
    "\n",
    "\n",
    "# --- Step 1: Select 4 input features (X) ---\n",
    "X_wind = df[percipitation_features]\n",
    "\n",
    "# --- Step 2: Select 3 target variables (Y) ---\n",
    "Y_wind = df['wind_speed_10m_mean (km/h)']\n",
    "\n",
    "# Split data into training and testing sets (80/20)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(\n",
    "    X_wind, Y_wind, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# Scale input features to range [-1, 1]\n",
    "xscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "X_wind_train_scaled = xscaler.fit_transform(X_train)\n",
    "X_wind_test_scaled = xscaler.transform(X_test)\n",
    "\n",
    "# Scale target values to range [-1, 1]\n",
    "yscaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "\n",
    "# Reshape target arrays to 2D for the scaler\n",
    "Y_wind_train_scaled = yscaler.fit_transform(Y_train.values.reshape(-1, 1))\n",
    "Y_wind_test_scaled = yscaler.transform(Y_test.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47543a86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
